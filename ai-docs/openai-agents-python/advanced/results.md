# Results

When you call the `Runner.run` methods, you either get a:

- [`RunResult`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResult) if you call `run` or `run_sync`
- [`RunResultStreaming`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultStreaming) if you call `run_streamed`

Both of these inherit from [`RunResultBase`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase), which is where most useful information is present.

## Final output

The [`final_output`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.final_output) property contains the final output of the last agent that ran. This is either:

- a `str`, if the last agent didn't have an `output_type` defined
- an object of type `last_agent.output_type`, if the agent had an output type defined.

**Note:** `final_output` is of type `Any`. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.

```python
from agents import Agent, Runner

# String output (default)
agent = Agent(name="Assistant", instructions="You are helpful")
result = await Runner.run(agent, "Hello")
print(result.final_output)  # str

# Structured output
from pydantic import BaseModel

class Analysis(BaseModel):
    sentiment: str
    confidence: float

agent = Agent(
    name="Analyzer",
    instructions="Analyze sentiment",
    output_type=Analysis
)
result = await Runner.run(agent, "I love this!")
output = result.final_output  # Analysis object
print(f"Sentiment: {output.sentiment}, Confidence: {output.confidence}")
```

## Inputs for the next turn

You can use [`result.to_input_list()`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.to_input_list) to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.

```python
# First turn
result = await Runner.run(agent, "What's the capital of France?")
print(result.final_output)  # "Paris"

# Second turn - manually managing conversation history
new_input = result.to_input_list() + [{"role": "user", "content": "What about Germany?"}]
result = await Runner.run(agent, new_input)
print(result.final_output)  # "Berlin"
```

## Last agent

The [`last_agent`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.last_agent) property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.

```python
triage_agent = Agent(
    name="Triage",
    instructions="Route to appropriate specialist",
    handoffs=[spanish_agent, french_agent]
)

result = await Runner.run(triage_agent, "Hola!")
print(f"Last agent: {result.last_agent.name}")  # "Spanish Agent"

# Use the same agent for the next turn
result2 = await Runner.run(result.last_agent, "¿Cómo estás?")
```

## New items

The [`new_items`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.new_items) property contains the new items generated during the run. The items are [`RunItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.RunItem)s. A run item wraps the raw item generated by the LLM.

### Item Types

- **[`MessageOutputItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.MessageOutputItem)**: A message from the LLM
- **[`HandoffCallItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.HandoffCallItem)**: The LLM called the handoff tool
- **[`HandoffOutputItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.HandoffOutputItem)**: A handoff occurred
- **[`ToolCallItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ToolCallItem)**: The LLM invoked a tool
- **[`ToolCallOutputItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ToolCallOutputItem)**: A tool was called and returned output
- **[`ReasoningItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ReasoningItem)**: Reasoning from the LLM

```python
result = await Runner.run(agent_with_tools, "Calculate something")

for item in result.new_items:
    if item.type == "message_output_item":
        print(f"Message: {item.content}")
    elif item.type == "tool_call_item":
        print(f"Tool called: {item.tool_name}")
    elif item.type == "tool_call_output_item":
        print(f"Tool output: {item.output}")
    elif item.type == "handoff_output_item":
        print(f"Handoff from {item.source_agent.name} to {item.target_agent.name}")
```

## Other information

### Guardrail results

The [`input_guardrail_results`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.input_guardrail_results) and [`output_guardrail_results`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.output_guardrail_results) properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store.

```python
result = await Runner.run(agent_with_guardrails, "User input")

# Check input guardrail results
if result.input_guardrail_results:
    for gr in result.input_guardrail_results:
        print(f"Input guardrail: {gr}")

# Check output guardrail results
if result.output_guardrail_results:
    for gr in result.output_guardrail_results:
        print(f"Output guardrail: {gr}")
```

### Raw responses

The [`raw_responses`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.raw_responses) property contains the [`ModelResponse`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ModelResponse)s generated by the LLM.

```python
result = await Runner.run(agent, "Hello")

for response in result.raw_responses:
    print(f"Model: {response.model}")
    print(f"Usage: {response.usage}")
```

### Original input

The [`input`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.input) property contains the original input you provided to the `run` method. In most cases you won't need this, but it's available in case you do.

```python
result = await Runner.run(agent, "Original question")
print(f"Original input: {result.input}")  # "Original question"
```

## Working with Typed Outputs

When an agent has an `output_type` defined, you can use `final_output_as()` to get typed access:

```python
from pydantic import BaseModel

class WeatherReport(BaseModel):
    temperature: float
    conditions: str
    humidity: int

weather_agent = Agent(
    name="Weather",
    instructions="Provide weather information",
    output_type=WeatherReport
)

result = await Runner.run(weather_agent, "What's the weather?")

# Type-safe access
report = result.final_output_as(WeatherReport)
print(f"Temperature: {report.temperature}°C")
print(f"Conditions: {report.conditions}")
print(f"Humidity: {report.humidity}%")
```

## Streaming Results

When using `run_streamed`, you get a `RunResultStreaming` which provides streaming capabilities:

```python
result = Runner.run_streamed(agent, "Tell me a story")

# Stream events as they occur
async for event in result.stream_events():
    if event.type == "raw_response_event":
        # Handle streaming tokens
        pass

# After streaming completes, access full results
print(result.final_output)
print(result.new_items)
```